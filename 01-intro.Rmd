# Introduction {#intro}

I have built a CovidShiny project (for tracking coronavirus accumulation and monitoring the mutation emerging), the basic web appearance is below. This shiny app was created using css, javascript extension under shiny framework. 

The previous work I have done are:

1. Download fasta sequences from Gisaid manually, this should be updated every month as the virus spreads. You can only download 10 thousand seqs each time, and the manual nature of downloading makes you tired easily; 

2. Preprocess the fasta seqs using seqkit(filter) and nucmer(alignment) under linux;

3. Prepare input for R (require two steps): merge records that has consecutive order in the qpos field (or the same qpos field in deletion senario), then annotate each records (D614G for mutation type, spike for full name, etc.) **Note: merge and annotation are the most time-consuming process in the whole procedure.**

4. The inputs are further manipulated in R (data wrangling using tidyverse, dplyr; plot using ggplot2, plotly; web built using shiny plus other extensions, etc.). For web appearance I customed some css in www/ folder, but I still do not know how to make the app compatible with mobile device.

5. In the CovidShiny app (for some reason, we can not make it open source now), users are encouraged to change the input parameters, interact with dynamic plot (generated using plotly.js, such as time-series plot) and submit their RT-qPCR assays to evaluate the primers efficiency (based on mutation occurrence). The basic structure is: 

```{r web-home, fig.cap='Overview of CovidShiny app', out.width='80%', fig.asp=.75, fig.align='center', echo = FALSE}

knitr::include_graphics(path = "images/web_home.png")

```


Despite the wonderful UI and server performance at first glance, I came to realize some challenges during development:

* **Process management**: R is single-threaded, it is tedious to achieve multiple-threading using shiny framework (particularly embarrassing when serving several users simutaneously), and the intensive computation in merge-annotation process (given up to 1 million seqs per month) largely contradicts with the real-time feature in the first place.

* **Caching**? The same query syntax should return the same results, how to manage the objects in memory?

* **Speed up and automation**: How to speed up merge-annotation process (1 million seqs needs up to several days computing)?
  + From R to C++? 
  + Parallel computation, consider java
  + Cron jobs on Github (workflow action)/Docker

* **Data curation**: How to fetch data, and the data format (fasta for raw seqs, json for metadata)
  + The data schemas: using relational database like MySQL, Oracle; Downloaded data could be automatically uploaded to database (update function of db, db allows concurrency)
  + Discovery: compare new data and old data, analyze the difference
  
