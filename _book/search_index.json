[["index.html", "Bioinformatics in practice: web development, program optimization and integration Chapter 1 Preface 1.1 Purpose for writing 1.2 About me", " Bioinformatics in practice: web development, program optimization and integration Shaoqian Ma 2021-06-18 Chapter 1 Preface This is my first book written in Markdown. You can download the github version locally and render it on your PC. This short book (actually, study notes) introduces some of my bioinformatics practice in cooperation with members of professional IT team. 1.1 Purpose for writing I am a big fan of R and computational biology. Research in computational biology should involve innovations that could tweak classic theory and transform it into real-life application (medical treatment, clinical trial, drug discovery, etc.). As global challenge right now is fighting against COVID-19, our team aimed to provide an integrative resource, not only focusing on SARS-CoV-2, but also applicable in future research regarding other viruses. Overall, I think a data schemas is instructive. I hope this notebook would develop into a resource that will grow and change over time as a living book (tracking my knowledge footprints). The bookdown package can be installed from CRAN or Github, I recommend downloading the lastest released instead of CRAN repos, since the CRAN version can sometimes make you frustrated (Do not ask how I know). install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) 1.2 About me I’m a big fan of R and data science, also interested in python, java (I plan to systematically learn it), front-end js and back-end SQL (I enjoy using dplyr in R, somehow like using SQL interface). I have developed two R packages either seriously( CovidMutations ) or for fun( chromseq ). I have writen some articles on jianshu platform last year (2020, when I was a junior student in Xiamen University), talking about some of my experiences in bioinformatics. For biological scope, I have a broad interest in developmental epigenomics, machine learning in biomedicine, and novel NGS methods (my undergraduate thesis titled: RNA-seq library construction using low cellular input and evaluation of single-cell data analysis pipeline). "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction I have built a CovidShiny project (for tracking coronavirus accumulation and monitoring the mutation emerging), the basic web appearance is below. This shiny app was created using css, javascript extension under shiny framework. The previous work I have done are: Download fasta sequences from Gisaid manually, this should be updated every month as the virus spreads. You can only download 10 thousand seqs each time, and the manual nature of downloading makes you tired easily; Preprocess the fasta seqs using seqkit(filter) and nucmer(alignment) under linux; Prepare input for R (require two steps): merge records that has consecutive order in the qpos field (or the same qpos field in deletion senario), then annotate each records (D614G for mutation type, spike for full name, etc.) Note: merge and annotation are the most time-consuming process in the whole procedure. The inputs are further manipulated in R (data wrangling using tidyverse, dplyr; plot using ggplot2, plotly; web built using shiny plus other extensions, etc.). For web appearance I customed some css in www/ folder, but I still do not know how to make the app compatible with mobile device. In the CovidShiny app (for some reason, we can not make it open source now), users are encouraged to change the input parameters, interact with dynamic plot (generated using plotly.js, such as time-series plot) and submit their RT-qPCR assays to evaluate the primers efficiency (based on mutation occurrence). The basic structure is: Figure 2.1: Overview of CovidShiny app Despite the wonderful UI and server performance at first glance, I came to realize some challenges during development: Process management: R is single-threaded, it is tedious to achieve multiple-threading using shiny framework (particularly embarrassing when serving several users simutaneously), and the intensive computation in merge-annotation process (given up to 1 million seqs per month) largely contradicts with the real-time feature in the first place. Caching? The same query syntax should return the same results, how to manage the objects in memory? Speed up and automation: How to speed up merge-annotation process (1 million seqs needs up to several days computing)? From R to C++? Parallel computation, consider java Cron jobs on Github (workflow action)/Docker Data curation: How to fetch data, and the data format (fasta for raw seqs, json for metadata) The data schemas: using relational database like MySQL, Oracle; Downloaded data could be automatically uploaded to database (update function of db, db allows concurrency) Discovery: compare new data and old data, analyze the difference "],["possible-solutions-for-the-project.html", "Chapter 3 Possible solutions for the project", " Chapter 3 Possible solutions for the project We discussed some senario and proposed several strategies: Nucmer alignment: I used nucmer script (MUMmer software, written in C++) to obtain hundreds of mutation records in SRAS-CoV-2 genome. In some cases it throws error when fasta file is incomplete (I don’t know the algorithm in detail). The output format .snps is not compatible with popular bioinformatic format like vcf. Given these, maybe recompiling is needed (The alignment speed is sufficient, so we will consider this problem later). Numba: A compiler for Python Functions: For merge-annotation process, Numba can even speed up the computation faster than C (it calls GPU). What’s more, the for-loop in R can be optimized to parallel computing, the algorithm needs revision (time complexity from O(N^3) to O(logN)). Multi-threading serve: I have considered some available options to scale up shiny (support more users access), like: Alternatives to scale shiny: I recommend this video on youtube. Consider ShinyProxy, which is based on docker framework: Figure 3.1: Framework of shinyproxy Not all software support concurrency (e.g, all nucmer script in Shell), we plan to upload the data table (in csv format) to database (MySQL, Oracle, etc.), which is compatible with automated update and more queries from users (index construction in database to support faster query, Ngnix for reverse proxy). Separation of front-end and backend: Front-end (vue, echart, hichart, plotly?), back-end (python, java, SQL), separation makes optimization easily. Use gzip encoding to optimize the site. Test is important! Profile each step: Sequence preprocess, SQL query, merge-annotation, front-end figures…, each step needs example data with different size (10 seqs, 1000 seqs, 10000 seqs, etc.). For alignment, the time difference should be quantified. Selenium for web scraping (download sequence data): From semi-automation to automation, simulate the human behavior. We found that the Gisaid web rendered download tables at the backend and returned it to front-end via ajax (Asynchronous JavaScript and XML), the dynamic nature of this web and the random time-lapse between operation make it difficult to test the code. Use css or xpath to locate the element (selectorGadget in Chrome browser) Cross platform calling: communication among R, java, python, the middleware can be python or java. "],["backend-methods.html", "Chapter 4 Backend methods 4.1 MySQL query 4.2 SpringBoot in java 4.3 Merge, annotation algorithm writen in java", " Chapter 4 Backend methods We describe our backend methods step by step in this chapter. We decided to use java platform as the middle-ware, which integrates SQL query, api port testing and json data generation (api exposed to front-end). 4.1 MySQL query The covid_annot table behind my shiny web is uploaded to MySQL database, total 16 columns (see below). Consider that CovidShiny mainly focuses on querying data from the backend, the first thing to improve users experience and start our project is putting the large dataset in csv format in db (previously it was loaded into memory, which is unapplicable to production-grade app). Note: The dataset uploaded should be checked seriously just in case (some missing rows and NA in not-null column). Due to the high frequency of update, incremental id (corresponding to each unique mutation record of virus) column was set. Next, I have to mention that the subset function used in my shiny framework can be pretty low-efficient when dealing large dataframe with up to hundreds of thousands rows (since R have to scanning the whole dataframe row by row to find all targets). So the subset syntax should be transformed into SQL query syntax (first in R, for test), for example: From: # this example subset by country,M_type (with top frequency), if(country != &quot;global&quot;){ covid_annot&lt;-covid_annot[covid_annot$country == country, ] covid_annot&lt;- covid_annot[covid_annot$M_type %in% names(head(sort(table(covid_annot$M_type),decreasing=TRUE),n=top)),] } if(country == &quot;global&quot;){ covid_annot&lt;- covid_annot[covid_annot$M_type %in% names(head(sort(table(covid_annot$M_type),decreasing=TRUE),n=top)),] } to: exec_sql_query_21&lt;-function(conn,args_protein_list,args_country_list,order_M_type){ if (args_country_list==&#39;global&#39;){ #global without force index, slow sql_sentence = glue(&quot;select sample, refpos, M_type from covid_annot \\ where M_type in (select M_type from (select M_type from covid_annot \\ where protein = &#39;{args_protein_list}&#39; \\ group by M_type order by count(M_type) DESC LIMIT {order_M_type})as t) \\ and protein = &#39;{args_protein_list}&#39;&quot;) } else{ sql_sentence = glue(&quot;select sample, refpos, M_type from covid_annot FORCE INDEX (country_index)\\ where M_type in (select M_type from (select M_type from covid_annot \\ where protein = &#39;{args_protein_list}&#39; and country = &#39;{args_country_list}&#39;\\ group by M_type order by count(M_type) DESC LIMIT {order_M_type})as t) \\ and protein = &#39;{args_protein_list}&#39; and country = &#39;{args_country_list}&#39;&quot;) } result = dbSendQuery(conn, sql_sentence) # read in result = fetch(result, n = -1) # return dataframe return(result) } result_global_21&lt;-exec_sql_query_21(my_conn,args_protein_list=&#39;S&#39;,args_country_list=&#39;global&#39;,order_M_type=&#39;3&#39;) SQL sentences need to be optimized, in a few aspects: Use count command to summarize the data feature, such as mutations per sample, frequency of each mutation type, etc. We do this because if we always import a large dataset from MySQL to middle-ware to calculate the statistics, it may run into out of memory. Construct index: We used forced index of country field, to faster M_type sorting (i.e., filter country first, then sort M_type). Divide complex SQL sentence into sub-query, intermediate query results can be preserved in middle-ware (java). I made several intermediate files for the test of SQL query. With simple modifications, these SQL sentences can be easily translated into java code. 4.2 SpringBoot in java The web framework is springboot, and we used swagger for api testing (from my perspective, the api testing is the most powerful part of java application, enabling separation of front-end and back-end. Shiny does not have this function). Figure 4.1: Swagger for best APIs The json data returned by api (simulate GET, POST requests): Figure 4.2: APIs return json 4.3 Merge, annotation algorithm writen in java The format of snps records output by nucmer script: Each row represents a single mutation record. However, some mutations are contiguous (e.g, 121:C -&gt; T, 122:A -&gt; C, 123:G -&gt; A). Also, contiguous mutations in some cases can be identified as deletions or insertions. For convenience (also save memory, reduce rows in the dataframe), I merged contiguous mutations into single records, rendering merged nucmer data, making it easy to annotate deletion and insertion events. I followed the code written by other publications(Mercatelli and Giorgi 2020). I interpreted this tedious code and expressed the core idea to an algorithm engineer. ### Merge neighboring events ---- samples&lt;-unique(nucmer$qname) length(samples) # 12822 pb&lt;-txtProgressBar(0,length(samples),style=3) for (pbi in 1:length(samples)){ # This will update the nucmer object, deal with each sample records sample&lt;-samples[pbi] allvars&lt;-nucmer[nucmer$qname==sample,] snps&lt;-allvars[(allvars[,&quot;rvar&quot;]!=&quot;.&quot;)&amp;(allvars[,&quot;qvar&quot;]!=&quot;.&quot;),] inss&lt;-allvars[(allvars[,&quot;rvar&quot;]==&quot;.&quot;),] dels&lt;-allvars[(allvars[,&quot;qvar&quot;]==&quot;.&quot;),] # Merge insertions prevqpos&lt;-0 prevrowname&lt;-NULL remove&lt;-c() i&lt;-1 corrector&lt;-0 while(i&lt;=nrow(inss)){ rpos&lt;-inss[i,&quot;rpos&quot;] rvar&lt;-inss[i,&quot;rvar&quot;] qvar&lt;-inss[i,&quot;qvar&quot;] qpos&lt;-inss[i,&quot;qpos&quot;] if((qpos!=1)&amp;(qpos==(prevqpos+1+corrector))){ inss&lt;-inss[-i,] inss[prevrowname,&quot;qvar&quot;]&lt;-paste0(inss[prevrowname,&quot;qvar&quot;],qvar) corrector&lt;-corrector+1 i&lt;-i-1 } else { corrector&lt;-0 prevrowname&lt;-rownames(inss)[i] prevqpos&lt;-qpos } i&lt;-i+1 } # Merge deletions prevqpos&lt;-0 prevrowname&lt;-NULL remove&lt;-c() i&lt;-1 while(i&lt;=nrow(dels)){ rpos&lt;-dels[i,&quot;rpos&quot;] rvar&lt;-dels[i,&quot;rvar&quot;] qvar&lt;-dels[i,&quot;qvar&quot;] qpos&lt;-dels[i,&quot;qpos&quot;] if((qpos!=1)&amp;(qpos==(prevqpos))){ dels&lt;-dels[-i,] dels[prevrowname,&quot;rvar&quot;]&lt;-paste0(dels[prevrowname,&quot;rvar&quot;],rvar) i&lt;-i-1 } else { prevrowname&lt;-rownames(dels)[i] prevqpos&lt;-qpos } i&lt;-i+1 } # Merge SNPs prevqpos&lt;-0 prevrowname&lt;-NULL remove&lt;-c() i&lt;-1 corrector&lt;-0 while(i&lt;=nrow(snps)){ rpos&lt;-snps[i,&quot;rpos&quot;] rvar&lt;-snps[i,&quot;rvar&quot;] qvar&lt;-snps[i,&quot;qvar&quot;] qpos&lt;-snps[i,&quot;qpos&quot;] if((qpos!=1)&amp;(qpos==(prevqpos+1+corrector))){ snps&lt;-snps[-i,] snps[prevrowname,&quot;rvar&quot;]&lt;-paste0(snps[prevrowname,&quot;rvar&quot;],rvar) snps[prevrowname,&quot;qvar&quot;]&lt;-paste0(snps[prevrowname,&quot;qvar&quot;],qvar) corrector&lt;-corrector+1 i&lt;-i-1 } else { corrector&lt;-0 prevrowname&lt;-rownames(snps)[i] prevqpos&lt;-qpos } i&lt;-i+1 } # Remerge back allvars2&lt;-rbind(snps,inss,dels) remove&lt;-setdiff(rownames(allvars),rownames(allvars2))#?setdiff nucmer&lt;-nucmer[setdiff(rownames(nucmer),remove),] nucmer[rownames(allvars2),]&lt;-allvars2 setTxtProgressBar(pb,pbi) } This chunk of code has several problems: Subsetting mutation records of each sample means exhaustive search, which will scan all rows in each for-loop. When it comes to solve hundreds of thousands rows, this algorithm can be extremely time-consuming. #This subsets all records of each sample allvars&lt;-nucmer[nucmer$qname==sample,] Each sample can be processed independently. However, the above code does not involve concurrency. To solve these problem, we rewrote the code into java, which supports concurrency easily: /** * first group and sort records by sample name, mutation type (insertion, deletion, snp), qpos, rpos, so java only needs to merge records in order, without tedious subsetting. * * @param ses * @return */ public List&lt;SnpsEntity&gt; mergeQpos(List&lt;SnpsEntity&gt; ses) { ses.sort((o1, o2) -&gt; { if (!StrUtil.equals(o1.getQ_name(), o2.getQ_name())) return StrUtil.compare(o1.getQ_name(), o2.getQ_name(), true); if (o1.getType() != o2.getType()) return Integer.compare(o1.getType().ordinal(), o2.getType().ordinal()); if (o1.getQ_pos() != o1.getQ_pos()) return Integer.compare(o1.getQ_pos(), o2.getQ_pos()); return Integer.compare(o1.getR_pos(), o2.getR_pos()); }); List&lt;SnpsEntity&gt; ans = new ArrayList&lt;&gt;(); for (int i = 0, j; i &lt; ses.size(); i = j) { SnpsEntity se = ses.get(i); boolean dbg = false; // dbg = se.getR_pos() == 28280 &amp;&amp; StrUtil.equals(se.getQ_name(), // &quot;hCoV-19/Germany/un-RKI-I-112884/2021|EPI_ISL_1977817|2021-04-23&quot;); if (dbg) System.out.println(se); for (j = i + 1; j &lt; ses.size() &amp;&amp; ses.get(j - 1).canMerge(ses.get(j)); j++) { var sj = ses.get(j); if (dbg) System.out.println(sj); if (se.getType() == SnpsType.INS) se.setQ_var(se.getQ_var() + sj.getQ_var()); else if (se.getType() == SnpsType.DEL) se.setR_var(se.getR_var() + sj.getR_var()); else { se.setR_var(se.getR_var() + sj.getR_var()); se.setQ_var(se.getQ_var() + sj.getQ_var()); } } ans.add(se); } ans.sort((o1, o2) -&gt; { if (!StrUtil.equals(o1.getQ_name(), o2.getQ_name())) return StrUtil.compare(o1.getQ_name(), o2.getQ_name(), true); return Integer.compare(o1.getR_pos(), o2.getR_pos()); }); return ans; } } References "],["app.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "]]
